\chapter{Aufwand}
Das Thema dieses Kapitels lässt sich mit einer Frage zusammenfassen:
Wie bemessen wir den Aufwand,
den es braucht ein Problem durch einen Algorithmus zu lösen?
In \autoref{einleitung} haben wir formale Sprachen als Repräsentation von Problemen eingeführt,
in \autoref{reg} haben wir gelernt, was ein Algorithmus ist und
in \autoref{turing} wurde eine Möglichkeit eingeführt,
alle bekannten Algorithmen zu implementieren:
die Turingmaschine.
Diesen Formalismus wollen wir in diesem Kapitel nutzen,
um genau zu spezifizieren,
was Aufwand bedeutet.

\section{Effizienz in Zeit (und Raum)}

Was meinen wir, wenn wir von Aufwand oder Effizienz einer Berechnung sprechen?
Meist wird als erstes die \emph{Zeit} genannt,
die es braucht,
damit eine Berechnung ein Ergebnis liefert.
Es wäre ebenso möglich,
den \emph{Speicherplatz}\footnote{
    Im Englischen sprechen wir von ``space'', daher die Überschrift des Abschnitts.}
als Aufwandsdimension zu verstehen,
den eine Implementierung benötigt,
um den Input,
eventuelle Zwischenergebnisse
und den Output zu speichern.
Beides, Zeit und ``Raum'' sind valide Größen,
anhand derer wir Aufwand oder Effizienz bemessen können.
Wir werden uns in diesem Skript auf Zeit-Effizienz beschränken,
weswegen wir bei Aufwand beziehungsweise Effizienz immer von
zeitlichem Aufwand beziehungsweise zeitlicher Effizienz sprechen.
Allerdings kann vieles auf Raum-Effizienz übertragen werden,
das wir in diesem Kapitel entwickeln.

Wir wollen über diese drei folgenden Schritte zu einem Verständnis von Aufwand zu kommen:
\begin{itemize}
    \item Aufwand für einen Lauf einer spezifischen Turingmaschine auf einem spezifischen Wort.
    \item Abschätzung für die Länge von Läufen einer spezifischen Turingmaschine
        auf allen Worten einer Sprache.
    \item Schranken für die Länge aller Turingmaschinen, die eine spezifische Sprache erkennen.
\end{itemize}

Die folgenden Abschnitte hangeln sich an diesen Schritten entlang.

\subsection{Aufwand eines spezfischen Laufs}
Am Anfang unserer Überlegung haben wir diese zwei Komponenten:
\begin{itemize}
    \item Die konkrete Problemstellung, gegeben als \emph{Wort $w \in L$}.
    \item Den konkreten Algorithmus,
        implementiert als \emph{Turingmaschine $tm \in TM$}.\footnote{
            $TM$ bezeichnet die Menge aller Turingmaschinen.}
\end{itemize}
Dies sind formal korrekt eingeführte Konzepte
und wir können Zeit und Platzaufwand daran genau definieren.
Gesucht ist also eine Funktion $\tau: L \times TM \rightarrow \mathbb{N}$,
die für ein Wort $w \in L$ auf einer bestimmten Turingmaschine $tm \in TM$
den zeitlichen Aufwand in einer natürlichen Zahl ausdrückt.

Glücklicherweise haben wir in \autoref{turing} bereits ein Konzept eingeführt,
dass wir hierfür nutzen können:
Die Länge des Laufes von $w$ auf $tm$:
$\tau: \Sigma^* \times TM \rightarrow \mathbb{N}$ ist die Funktion,
die für ein Wort $w \in \Sigma^*$ den Zeitaufwand auf einer Turingmaschine TM angibt.
Wir legen fest: $\tau(w,tm) =  length(l_{w,tm})$.
Für ein Wort $w \in L$ ergibt sich eine Folge von Schnappschüssen,
dessen Länge wir als zeitlichen Aufwand definieren.



\subsection{Abschätzung des Aufwandes aller Läufe}

Bisher haben wir aber nur über den Aufwand gesprochen,
den ein \emph{Wort} verursacht,
nicht aber über den Aufwand,
den ein ganzes Problem, also eine Menge von Wörtern,
also eine \emph{formale Sprache} verursacht.
Wenn wir die Effizienz einer Problemlösung angeben,
haben wir prinzipiell drei Szenarien im Kopf:
\begin{itemize}
    \item \textbf{Best Case}: Wir wählen das Wort aus der formalen Sprache aus,
        für das der Algorithmus den kürzesten Lauf hat,
        also die geringste Zeit benötigt.
        Wir betrachten also das Minimum von $\tau$.
    \item \textbf{Average Case}: Wir nutzen statistische Methoden,
        um die wahrscheinliche Länge eines Laufs zu quantifizieren.
        Wir betrachten also zum Beispiel den Erwartungswert von $\tau$,
        wenn wir per Zufall ein Wort aus L auswählen.\footnote{
            Der Modus (entspricht dem Median in einer endlichen Stichprobe) wäre eine Alternative.
        Mit der Standardabweichung (Streuungsmaß) oder einer Kombination mehrerer Maße,
        verbessert sich die Aussagekraft,
        was aber die einfache Vergleichbarkeit von Algorithmen wiederum erschwert.}
    \item \textbf{Worst Case}: Wir wählen das Wort aus der formalen Sprache aus,
        für das der Algorithmus den längsten Lauf hat,
        also die meiste Zeit benötigt.
        Wir betrachten also das Maximum von $\tau$. 
\end{itemize} 

Da der Best Case meist wenig Aussagekraft über die Güte des Algorithmus besitzt,
wird er selten als Maß für den Aufwand eines Problems genutzt.
Am aussagekräftigsten ist der Average Case,
aber um ihn formal sauber anzuwenden braucht es
entsprechendes statistisches Werkzeug.\footnote{
    Für interessierte Leser:innen sei \cite{knuth1}, 96ff bzw. 1.2.10 empfohlen,
    hier spielt Knuth die probabilistische Analyse eines recht simplen Algorithmus durch.} 
Da dies den Rahmen des Skriptes sprengen würde,
konzentrieren wir uns auf den Worst Case.
Wir vereinbaren also: Wenn wir vom Aufwand eines Problems sprechen,
sprechen wir dabei stets vom Worst Case.

Ein verbleibendes Problem: Minimum, Maximum und die statistischen Maße 
können für eine (unendliche) Sprache gar nicht definiert sein.
Was ist zum Beispiel das Maximium von $\tau$ auf EVEN und der
dazu in \autoref{turing} dargestellten Turingmaschine?
Um also sinnvoll von Aufwand zu sprechen müssen wir die \emph{Länge},
also $|w|$ in Verhältnis zum Aufwand setzen:
Sei $t_{tm}: \mathbb{N} \rightarrow \mathbb{N}$ so definiert:
$t_{tm}(n) = max({\tau(w, tm)| |w| = n})$
$t_{tm}$ gibt also die jeweilige Worst Case Laufzeit für ein Wort der Länge $n$ an.
Dies erlaubt es uns auch über unendlich große formale Sprachen Aussagen zu treffen,
indem wir angeben, wie sich der Aufwand bei steigender Inputgröße entwickelt.

\subsection{Untere und Obere Schranken für den Aufwand}
Mit $t_{tm}$ und dem Worst Case-Wert können wir Aussagen über den Aufwand treffen,
den ein bestimmter Algorithmus für ein bestimmtes Problem verursacht.
Wir treffen allerdings keine Aussagen darüber,
wie viel Aufwand das Problem \emph{an sich} erzeugt.
Dazu müssten wir Aussagen über alle prinzipiell \emph{möglichen} Algorithmen,
also Turingmaschinen treffen,
die das Problem lösen:
Den Aufwand eines Problems würden wir dann mit dem effizientestem Algorithmus gleichsetzen.
Oftmals ist der Nachweis sehr schwer,
ob es (k)einen effizienteren Algorithmus
für ein Problem gibt.
Den aktuellen Stand der Forschung erlernt man typischer Weise in Vorlesungen über
Algorithmen und Datenstrukturen.

%Ein Beispiel hierfür ergibt sich aus $U\textsubscript{ARTICLESORT}$.
%Fassen wir dieses Problem informell so auf $I\textsubscript{ARTICLESORT}$:
%\begin{itemize}
%    \item Gegeben ein Tupel von Zahlen
%    \item Gesucht: die Permutatio des Tupels, die absteigend sortiert ist
%\end{itemize}
%So gibt es viele Algorithmen, die dieses Problem lösen.
%Für vergleichbasierte Sortieralgorithmen lässt sich eine untere Schranke finden. 

\section{Landau und Big-O: Wachstum}

$t_{tm}$ ist eine Möglichkeit,
die Aufwandsentwicklung für steigende Inputgrößen formal zu spezfifizieren.
Für die meisten Fälle ist $t_{tm}$ aber zu feingranular:
Wir interessieren uns beispielsweise nur,
ob das Laufzeitverhalten sinnvoll beschränken lässt,
d.h. ob es eine asymptotische obere oder untere Schranke für den Aufwand gibt.

Dies wird typischerweise mit der Landau-Notation angeben.
Wir definieren eine Menge von Funktionen in Abhängigkeit einer oberen Schranke
und lassen dabei Faktoren außen vor,
die für die Wachstumsdynamik unwesentlich sind:

\[
O(g(n)) = \{f|\forall c \exists n_0 \forall n (n \geq n_0 \rightarrow f(n) \leq c \cdot g(n))\}
\]

TODO: Ergänze andere Klassen (o, $\theta$ usw).

\section{Messen vs. Beweisen}\label{messenVsBeweisen}

Messen ist schwierig:
\begin{itemize}
  \item Man kann nur den Zeit- oder Platzaufwand von Implementierungen via Messung ermitteln
  \item Man kann den Aufwand von Algorithmen oder gar Probleme nicht via Messung ermitteln!
  \item Die Menge der möglichen Inputs vieler Algorithmen ist unendlich (Stichproben + Statistiken)
  \item Das „Rauschen“ bei Messungen muss berücksichtigt werden:
  \item Genutzte Hardware
  \item Genutztes Betriebssystem
  \item Spezifische Versionen (z.B. Compiler, Browser, OS)
  \item Lastsituation auf der Maschine
  \item Virtualisierung
  \item Netz
\end{itemize}

Aber: Theoretische Annahmen:
\begin{itemize}
  \item Operationen haben identischen Aufwand
  \item Daten und Zwischenergebnisse nutzen identischen Speicherplatz
  \item Der Input kann unendlich wachsen
  \item Ist jeder Input gleich wahrscheinlich?
\end{itemize}

\section{Relativer Aufwand}
Hier Diskussion Sortierungsalgorithmen.
\begin{itemize}
    \item Relativ zur Implementierungssprache (Mehrbandturingmaschinen, Registermaschinen)
    \item Relativ zur Datenrepräsentation (Größe des Alphabets, Art der Datenstrukturen)
\end{itemize}
